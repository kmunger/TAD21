docvars(data_corpus_nobel, "Date")
docvars(data_corpus_nobel)
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "year"))
nobel_year <- (docvars(data_corpus_nobel, "year"))
install.packages("randomizr", repos="http://r.declaredesign.org")
library("randomizr", lib.loc="~/R/win-library/3.4")
remove.packages("randomizr", lib="~/R/win-library/3.4")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr", repos = "http://r.declaredesign.org")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr")
library("randomizr", lib.loc="~/R/win-library/3.4")
library(DeclareDesign)
library(quanteda)
library(sophistication)
data("data_corpus_Crimson")
library(dplyr)
require(stringr)
require(data.table)
##read in
load("C:/Users/kevin/Documents/GitHub/sophistication-papers/analysis_article/output/fitted_BT_model.Rdata")
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
data("data_corpus_SOTU")
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
data_corpus_SOTU
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
library(sophistication)
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
library(quanteda.quanteda)
library(quanteda.corpora)
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora", force = TRUE)
install.packages("digest")
library(readtext)
xx<-readtext(xx.txt)
xx<-readtext("xx.txt")
xx<-readtext(file ="xx.txt")
?readtext
xx<-readtext(file ="xx")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv", stringsAsFactors = F)
mturk<-read.csv("C:/Users/kevin/desktop/DLS.csv", stringsAsFactors = F)
library(tidyverse)
library(lme4)
mydata$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1
mturk$Race[mturk$Race |= "White" &  mturk$Race |= "Black or African American" & mturk$Race |= "Asian"]<-"Other"
mturk$Race[mturk$Race != "White" &  mturk$Race != "Black or African American" & mturk$Race != "Asian"]<-"Other"
mturk$Race
mturk$Race[mturk$Race == "White" &  mturk$Hispanic == "Yes"]<-"Hispanic"
mturk$Race
mturk$Overall..how.confident.do.you.feel.using.computers..smartphones..or.other.electronic.devices.to.do.the.things.you.need.to.do.online..
mturk[38,]
mturk$Race
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("Rcpp")
install.packages("Rcpp")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("digest")
install.packages("digest")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("rlang")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("testthat")
install.packages("testthat")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("backports")
install.packages("backports")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("processx")
install.packages("processx")
install.packages("ps")
install.packages("ps")
install.packages("ISOcodes")
install.packages("usethis")
install.packages("curl")
install.packages("fs")
install.packages("gh")
install.packages("git2r")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("quanteda")
devtools::install_github("kbenoit/sophistication", force=T)
library(quanteda)
devtools::install_github("kbenoit/sophistication", force=T)
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("ellipsis")
install.packages("ellipsis")
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("glue")
install.packages("glue")
install.packages("vctrs")
devtools::install_github("benjaminguinaudeau/tiktokr")
library(tiktokr)
library(reticulate)
use_python(py_config()$python)
Y
yes
use_python(py_config()$python)
install_tiktokr()
init_tiktokr()
trends <- get_trending(200)
user <- get_username("willsmith")
get_username("willsmith")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("glue")
install.packages("glue")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("rlang")
install.packages("rlang")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("processx")
install.packages("processx")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("fansi")
install.packages("fansi")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("ps")
install.packages("ps")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
library(RBERT)
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased"
)
text_to_process <- c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse.",
"An impulse is like a push.",
"Impulse is force times time.")
text_to_process2 <- list(c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse."),
c("An impulse is like a push.",
"Impulse is force times time."))
BERT_feats <- extract_features(
examples = text_to_process2,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
output_vector1 <- BERT_feats$output %>%
dplyr::filter(
sequence_index == 1,
token == "[CLS]",
layer_index == 12
) %>%
dplyr::select(dplyr::starts_with("V")) %>%
unlist()
output_vector1
install_tensorflow()
tensorflow::install_tensorflow(version = "1.13.1")
text_to_process <- c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse.",
"An impulse is like a push.",
"Impulse is force times time.")
text_to_process2 <- list(c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse."),
c("An impulse is like a push.",
"Impulse is force times time."))
BERT_feats <- extract_features(
examples = text_to_process2,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
devtools::install_github("rstudio/reticulate")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
#This also works for corpus objects:
summary(corpus(data_char_ukimmig2010))
#To access the **syllables** of a text, we use `syllables()`:
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
#We can even compute the **Scabble value** of English words, using `scrabble()`:
nscrabble(c("cat", "quixotry", "zoo"))
#We can analyze the **lexical diversity** of texts, using `lexdiv()` on a dfm:
summary(data_corpus_inaugural)
?quanteda
package_version("quanteda")
package_version(quanteda)
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
textstat_lexdiv(myDfm, "R")
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
install.packages("quanteda.textstats")
library(nsyllable)
library(quanteda.textstats)
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
?textstat_lexdiv
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
readab
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan"],  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
textstat_simil(presDfm, presDfm[c("2009-Obama", "2013-Obama").], margin = "documents", method = "cosine")
textstat_simil(presDfm, presDfm["2009-Obama": "2013-Obama",], margin = "documents", method = "cosine")
textstat_simil(presDfm, presDfm[c("2009-Obama": "2013-Obama"),], margin = "documents", method = "cosine")
textstat_simil(presDfm, presDfm[c("2009-Obama", "2013-Obama"),], margin = "documents", method = "cosine")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
presDfm <- dfm(data_corpus_inaugural)
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
plot(presCluster)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
corpus_txt<-corpus(txt)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
corpus_txt<-corpus(txt)
dfm_txt<-dfm(corpus_txt)
data("data_corpus_inaugural")
df <- texts(data_corpus_inaugural)
df[1]
toks <- tokens(data_corpus_inaugural)
tokenz <- lengths(toks)
tokenz
typez <- ntype(df)
ttr <- typez / tokenz
plot(ttr)
aggregate(ttr, by = list(docvars(data_corpus_inaugural)$Party), FUN = mean)
textstat_lexdiv(dfm(data_corpus_inaugural, groups = "President", verbose = FALSE))
textstat_readability(data_corpus_inaugural, "Flesch")
corr_matrix<-textstat_readability(data_corpus_inaugural, c("Flesch", "Dale.Chall", "SMOG", "Coleman.Liau" ))
corr_matrix
corr_matrix_nums<- corr_matrix[,c(2:5)]
cor(corr_matrix_nums)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
corp<-corpus(txt)
corp
dfm1<-dfm(corp)
dfm2<-dfm(corp,    remove = stopwords("english"))
lexdiv1<-textstat_lexdiv(dfm1, measure = "TTR")
lexdiv2<-textstat_lexdiv(dfm2, measure = "TTR")
lexdiv1
lexdiv2
## load rtweet package
library(rtweet)
app_name <- "smapp"
## copy and pasted *your* keys (these are fake)
consumer_key <- "5y3RgYfTUOUWOGheskv4p7WWO"
consumer_secret <- "j5pszDQ4bddt4pLiwqwezuETPYi3f1PyugM3L4eJdxUVsdwIco"
access_token <- "89098361-YS0MquarADIDh5XvrMdMzASxzxy3apOEgzTpKZvPY"
access_secret <- "jhZgEZuBS1mzDvMIRAhjNb9aklVN4S6UoCP7eL8l3GGIM"
## create token
token <- create_token(
app = app_name,
consumer_key = consumer_key,
consumer_secret = consumer_secret,
access_token = access_token,
access_secret = access_secret
)
## print token
token
get_token()
stream_tweets(
"joebiden,biden",
timeout = 5 , #60 seconds
file_name = "tweetsaboutbiden.json",
parse = FALSE, token = token
)
biden <- parse_stream("tweetsaboutbiden.json")
stream_tweets(
"joebiden,biden",
timeout = 60 , #60 seconds
file_name = "tweetsaboutbiden.json",
parse = FALSE, token = token
)
biden <- parse_stream("tweetsaboutbiden.json")
biden <- parse_stream("tweetsaboutbiden.json")
rt <- stream_tweets("",  token = token, timeout = 3)
ts_plot(rt, by = "secs")
View(rt)
table(rt$lang)
rt <- stream_tweets(lookup_coords("london, uk"),  token = token, timeout = 3)
cnn_fds <- get_friends("cnn", token = token)
View(cnn_fds)
cnn_fds_data <- lookup_users(cnn_fds$user_id, token = token)
View(cnn_fds_data)
x<-users_data(cnn_fds_data)
View(x)
cnn_flw <- get_followers("cnn", n = 750, token = token)
cnn_flw_data <- lookup_users(cnn_flw$user_id, token = token)
x<-users_data(cnn_flw_data)
x
View(x)
cnn <- lookup_users("cnn", token = token)
cnn$followers_count
tmls <- get_timelines(c("cnn", "BBCWorld", "foxnews"), n = 3200, token = token)
View(tmls)
tmls %>%
#  dplyr::filter(created_at > "2017-10-29") %>%
dplyr::group_by(screen_name) %>%
ts_plot("days", trim = 1L) +
ggplot2::geom_point() +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.title = ggplot2::element_blank(),
legend.position = "bottom",
plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of Twitter statuses posted by news organization",
subtitle = "Twitter status (tweet) counts aggregated by day",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
tay <- get_favorites("taylorswift13", n = 3, token = token)
tay
View(tay)
usrs <- search_users("PSU", n = 10, token = token)
x<-users_data(usrs)
View(x)
sf <- get_trends("san francisco", token = token)
sf
View(sf)
names(rt)
colnames(rt)
colnames(tmls)
rm(list = ls())
libraries <- c("topicmodels", "dplyr", "stm", "quanteda")
setwd("C:/Users/kevin/Documents/GitHub/TAD21")
install.packages("rsvd")
data(poliblog5k)
libraries
lapply(libraries, require, character.only = TRUE)
data(poliblog5k)
head(poliblog5k.meta)
head(poliblog5k.voc)
?stm
system.time(
blog_stm <- stm(poliblog5k.docs, poliblog5k.voc, 3, prevalence = ~rating + s(day), data = poliblog5k.meta))
plot(blog_stm, type = "labels")
plot(blog_stm, type = "summary")
plot(blog_stm, type="perspectives", topics = c(1,2))
prep <- estimateEffect(1:3 ~ rating + s(day) , blog_stm, meta = poliblog5k.meta)
plot(prep, "day", blog_stm, topics = c(1,2),
method = "continuous", xaxt = "n", xlab = "Date")
plot(prep, "rating", model = blog_stm,
method = "difference", cov.value1 = "Conservative", cov.value2 = "Liberal")
libraries <- c("topicmodels", "dplyr", "stm", "quanteda")
lapply(libraries, require, character.only = TRUE)
data(poliblog5k)
head(poliblog5k.meta)
head(poliblog5k.voc)
?stm
system.time(
blog_stm <- stm(poliblog5k.docs, poliblog5k.voc, 3, prevalence = ~rating + s(day), data = poliblog5k.meta))
plot(blog_stm, type = "labels")
plot(blog_stm, type = "summary")
plot(blog_stm, type="perspectives", topics = c(1,2))
prep <- estimateEffect(1:3 ~ rating + s(day) , blog_stm, meta = poliblog5k.meta)
plot(prep, "day", blog_stm, topics = c(1,2),
method = "continuous", xaxt = "n", xlab = "Date")
plot(prep, "rating", model = blog_stm,
method = "difference", cov.value1 = "Conservative", cov.value2 = "Liberal")
setwd("C:/Users/kevin/Documents/GitHub/TAD21")
# Load data
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
# Create date vectors
blm_tweets$datetime <- as.POSIXct(strptime(blm_tweets$created_at, "%a %b %d %T %z %Y",tz = "GMT")) # full date/timestamp
blm_tweets$date <- mdy(paste(month(blm_tweets$datetime), day(blm_tweets$datetime), year(blm_tweets$datetime), sep = "-")) # date only
# Collapse tweets so we are looking at the total tweets at the day level
blm_tweets_sum <- blm_tweets %>% group_by(date) %>% summarise(text = paste(text, collapse = " "))
# Remove non ASCII characters
blm_tweets_sum$text <- stringi::stri_trans_general(blm_tweets_sum$text, "latin-ascii")
# Removes solitary letters
blm_tweets_sum$text <- gsub(" [A-z] ", " ", blm_tweets_sum$text)
# As always we begin with a DFM.
# Create DFM
blm_dfm <-dfm(blm_tweets_sum$text, stem = F, remove_punct = T, tolower = T, remove_twitter = T, remove_numbers = TRUE, remove = c(stopwords("english"), "http","https","rt", "t.co"))
libraries <- c("ldatuning", "topicmodels", "ggplot2", "dplyr", "rjson", "quanteda", "lubridate", "parallel", "doParallel", "tidytext", "stringi", "tidyr")
lapply(libraries, require, character.only = TRUE)
# Load data
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
# Create date vectors
blm_tweets$datetime <- as.POSIXct(strptime(blm_tweets$created_at, "%a %b %d %T %z %Y",tz = "GMT")) # full date/timestamp
blm_tweets$date <- mdy(paste(month(blm_tweets$datetime), day(blm_tweets$datetime), year(blm_tweets$datetime), sep = "-")) # date only
# Collapse tweets so we are looking at the total tweets at the day level
blm_tweets_sum <- blm_tweets %>% group_by(date) %>% summarise(text = paste(text, collapse = " "))
# Remove non ASCII characters
blm_tweets_sum$text <- stringi::stri_trans_general(blm_tweets_sum$text, "latin-ascii")
# Removes solitary letters
blm_tweets_sum$text <- gsub(" [A-z] ", " ", blm_tweets_sum$text)
# As always we begin with a DFM.
# Create DFM
blm_dfm <-dfm(blm_tweets_sum$text, stem = F, remove_punct = T, tolower = T, remove_twitter = T, remove_numbers = TRUE, remove = c(stopwords("english"), "http","https","rt", "t.co"))
blm_dfm<-dfm_trim(blm_dfm, min_docfreq = 30)
s<-stm(blm_dfm, k = 0)
s<-stm(blm_dfm,
s<-stm(blm_dfm, K = 0)
plot(s, type = "summary")
plot(s, type="perspectives", topics = c(26,4))
blm_dfm <-dfm(blm_tweets_sum$text, stem = F, remove_punct = T, tolower = T, remove_twitter = T, remove_numbers = TRUE, remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
blm_dfm<-dfm_trim(blm_dfm, min_docfreq = 30)
s<-stm(blm_dfm, K = 0)
# A summary plot of the topics that ranks them by their average proportion in the corpus
plot(s, type = "summary")
plot(s, type="perspectives", topics = c(28,7))
library(rtweet)
app_name <- "smapp"
## copy and pasted *your* keys (these are fake)
consumer_key <- "5y3RgYfTUOUWOGheskv4p7WWO"
consumer_secret <- "j5pszDQ4bddt4pLiwqwezuETPYi3f1PyugM3L4eJdxUVsdwIco"
access_token <- "89098361-YS0MquarADIDh5XvrMdMzASxzxy3apOEgzTpKZvPY"
access_secret <- "jhZgEZuBS1mzDvMIRAhjNb9aklVN4S6UoCP7eL8l3GGIM"
## create token
token <- create_token(
app = app_name,
consumer_key = consumer_key,
consumer_secret = consumer_secret,
access_token = access_token,
access_secret = access_secret
)
## print token
token
get_token()
tay <- get_favorites("zendaya", n = 3, token = token)
z <- get_favorites("zendaya", n = 3, token = token)
z <- get_favorites("Zendaya", n = 3, token = token)
tmls <- get_timelines(c("Zendaya"), n = 500, token = token)
x<-dfm(tmls, remove = "stopwords")
x<-dfm(tmls$text, remove = "stopwords")
summary(x)
topfeatures(x)
blm_dfm <-dfm(tmls$text, stem = F, remove_punct = T, tolower = T, remove_twitter = T, remove_numbers = TRUE, remove = c(stopwords("english"),"¦","€","¥", "¯","~", "http","https","rt", "t.co"))
topfeatures(blm_dfm)
